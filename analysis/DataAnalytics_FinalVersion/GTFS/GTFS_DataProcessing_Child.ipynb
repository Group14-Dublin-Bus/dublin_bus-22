{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff2dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import pandas, numpy, matplotlib, seaborn libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ad5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve the data of corresponding line of bus\n",
    "list_id15=df_GTFS['trip_id_new'].loc[df_GTFS['LineId']==data].unique().tolist()\n",
    "df_GTFS_15 = df_GTFS[df_GTFS['trip_id_new'].isin(list_id15)]\n",
    "\n",
    "#we save the unique departure time in df_GTFS_15\n",
    "df_GTFS_15_deptime_list = df_GTFS_15['departure_time'].unique().tolist()\n",
    "df_stoptime_15 = df_stoptimes_new[df_stoptimes_new['trip_id_new'].isin(list_id15)]\n",
    "\n",
    "#we will save the trip_id for the departure_time matches in the df_GTFS_15_deptime_list\n",
    "trip_id_list = df_stoptime_15['trip_id'].loc[df_stoptime_15['departure_time'].isin(df_GTFS_15_deptime_list)].tolist()\n",
    "\n",
    "#We create column 'StartDate' in df_stoptime_15\n",
    "df_stoptime_15['StartDate'] = ''\n",
    "df_GTFS_15['StartDate']=df_GTFS_15['StartDate'].astype(str)\n",
    "\n",
    "#we add merge StartDate to the 'df_stoptime_15'\n",
    "list_for_concat = []\n",
    "for i in trip_id_list:\n",
    "    start_timevalue = df_stoptime_15['departure_time'].loc[(df_stoptime_15['trip_id']==i)&(df_stoptime_15['stop_sequence']==1)].values[0]\n",
    "    trip_id_new_value = df_stoptime_15['trip_id_new'].loc[(df_stoptime_15['trip_id']==i)&(df_stoptime_15['stop_sequence']==1)].values[0]\n",
    "    startdate_list = df_GTFS_15['StartDate'].loc[(df_GTFS_15['departure_time']==start_timevalue)&(df_GTFS_15['trip_id_new']==trip_id_new_value)].unique().tolist()\n",
    "    #print(startdate_list)\n",
    "    \n",
    "    for j in startdate_list:\n",
    "        df_stoptime_15['StartDate'].loc[(df_stoptime_15['trip_id']==i)] = j\n",
    "        list_for_concat.append(df_stoptime_15.loc[df_stoptime_15['StartDate']==j])\n",
    "\n",
    "if len(list_for_concat)==0:\n",
    "    print('None date in GTFS could match with the data in stop_times')\n",
    "else:\n",
    "    df_stoptime_15_new = pd.concat(list_for_concat)\n",
    "\n",
    "    #Now we will merger the data of df_stoptime_15_new and df_GTFS_15\n",
    "    df_stoptimes_merged = pd.merge(df_stoptime_15_new,df_GTFS_15[['trip_id_new','StartDate','stop_sequence','stop_id','Arr_delay','Dep_delay','LineId']],how=\"left\", on=['trip_id_new','StartDate','stop_sequence','stop_id'])\n",
    "    df_stoptimes_merged['Arr_delay'].fillna(0, inplace=True)\n",
    "    df_stoptimes_merged['Dep_delay'].fillna(0, inplace=True)\n",
    "    df_stoptimes_merged['LineId'].fillna(15, inplace=True)\n",
    "\n",
    "    #convert the data type of time into datetime\n",
    "    df_stoptimes_merged['StartDate']=df_stoptimes_merged['StartDate'].astype('datetime64')\n",
    "    df_stoptimes_merged['date_of_service'] = df_stoptimes_merged['StartDate'].dt.strftime(\"%Y-%m-%d\")\n",
    "    df_stoptimes_merged['arrival_time'] = df_stoptimes_merged['date_of_service'] + str(' ') + df_stoptimes_merged['arrival_time']\n",
    "    df_stoptimes_merged['departure_time'] = df_stoptimes_merged['date_of_service'] + str(' ') + df_stoptimes_merged['departure_time']\n",
    "\n",
    "    df_stoptimes_merged['arrival_time'] = df_stoptimes_merged['arrival_time'].astype('datetime64')\n",
    "    df_stoptimes_merged['departure_time'] = df_stoptimes_merged['departure_time'].astype('datetime64')\n",
    "\n",
    "    #calcuate the actual depart time and actual arrive time\n",
    "    df_stoptimes_merged['actual_arrival_time'] = 0\n",
    "    df_stoptimes_merged['actual_departure_time'] = 0\n",
    "    for i in range(len(df_stoptimes_merged)):\n",
    "        df_stoptimes_merged['actual_arrival_time'][i] = df_stoptimes_merged['arrival_time'][i] + pd.Timedelta(seconds=df_stoptimes_merged['Arr_delay'][i])\n",
    "        df_stoptimes_merged['actual_departure_time'][i] = df_stoptimes_merged['departure_time'][i] + pd.Timedelta(seconds=df_stoptimes_merged['Dep_delay'][i])\n",
    "    print('Finished with calculating actual depart time and actual arrive time')\n",
    "\n",
    "    #convert the data type of time into datetime\n",
    "    df_stoptimes_merged['actual_arrival_time'] = df_stoptimes_merged['actual_arrival_time'].astype('datetime64')\n",
    "    df_stoptimes_merged['actual_departure_time'] = df_stoptimes_merged['actual_departure_time'].astype('datetime64')\n",
    "\n",
    "    #Calculate the accumulated travel time\n",
    "    df_stoptimes_merged['acc_travel_time'] = (df_stoptimes_merged['actual_arrival_time'] - df_stoptimes_merged['StartDate'])\n",
    "    df_stoptimes_merged['acc_travel_time'] = df_stoptimes_merged['acc_travel_time'].dt.total_seconds()\n",
    "\n",
    "    #Create the following new features for modeling\n",
    "    df_stoptimes_merged['hour_of_day']=df_stoptimes_merged['actual_arrival_time'].dt.hour\n",
    "    df_stoptimes_merged['day_of_week']=df_stoptimes_merged['actual_arrival_time'].dt.dayofweek\n",
    "    df_stoptimes_merged['month_of_year']=df_stoptimes_merged['actual_arrival_time'].dt.month\n",
    "\n",
    "    #Prepare data for merging with weather info\n",
    "    df_stoptimes_merged['date']=df_stoptimes_merged['actual_arrival_time'].astype('datetime64[h]')\n",
    "    df_stoptimes_merged=pd.merge(df_stoptimes_merged,weather[['date','temp','rain']],how=\"left\",on=['date'])\n",
    "\n",
    "    #Clean up the columns\n",
    "    df_stoptimes_merged_cleaned = df_stoptimes_merged[['trip_id','arrival_time','stop_id','stop_sequence','stop_headsign','trip_id_new','StartDate','LineId','actual_arrival_time','actual_departure_time','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']]\n",
    "    df_stoptimes_merged_cleaned['temp'].fillna(0, inplace=True)\n",
    "    df_stoptimes_merged_cleaned['rain'].fillna(0, inplace=True)\n",
    "    df_stoptimes_merged_cleaned.to_csv('df_stoptimes_route_{}.csv'.format(lineid),index=False)\n",
    "    \n",
    "    if len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()) == 1:\n",
    "        print('Only one headsign or Nan value in headsign column.')\n",
    "\n",
    "        # #Select the features we need for modeling and Copy the original version of dataset\n",
    "        df_features_for_ML1 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']]\n",
    "        # df_features_for_ML['stop_headsign'].fillna('n/a',inplace=True)\n",
    "        # df_features_for_ML['temp'].fillna(0, inplace=True)\n",
    "        # df_features_for_ML['rain'].fillna(0, inplace=True)\n",
    "        # df_features_for_ML['stop_headsign'].astype('category')\n",
    "\n",
    "        # # set up dummies features\n",
    "        # df_features_for_ML = pd.get_dummies(df_features_for_ML, drop_first=False)\n",
    "\n",
    "        #Setting up the train/test split\n",
    "        # y is the target\n",
    "        y = df_features_for_ML1[\"acc_travel_time\"]\n",
    "        # X is everything else\n",
    "        X = df_features_for_ML1.drop([\"acc_travel_time\"],1)\n",
    "        # Split the dataset into two datasets: 70% training and 30% test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "        # Train RF with 10 trees\n",
    "        rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "        #Predict the test dataset\n",
    "        rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "        #Calculate the metrics\n",
    "        MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "        RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "        R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "        # writing these evaluation scores to a file\n",
    "        f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "        f.write(\"\\nLine: \" + str(data)  + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "        f.close()\n",
    "\n",
    "        # save the model to pickle file\n",
    "        filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}.pkl\".format(lineid)\n",
    "        pickle.dump(rfc, open(filename, 'wb'))\n",
    "    \n",
    "    elif len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()) > 1:\n",
    "        print('Length of headsign: ', len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()))\n",
    "\n",
    "        headsign1 = df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()[0]\n",
    "        headsign2 = df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()[1]\n",
    "\n",
    "        #Select the features we need for modeling and Copy the original version of dataset\n",
    "        df_features_for_ML1 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']].loc[df_stoptimes_merged_cleaned['stop_headsign']==headsign1]\n",
    "        df_features_for_ML2 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']].loc[df_stoptimes_merged_cleaned['stop_headsign']==headsign2]\n",
    "        # df_features_for_ML['stop_headsign'].fillna('n/a',inplace=True)\n",
    "        # df_features_for_ML['temp'].fillna(0, inplace=True)\n",
    "        # df_features_for_ML['rain'].fillna(0, inplace=True)\n",
    "        # df_features_for_ML['stop_headsign'].astype('category')\n",
    "\n",
    "        # # set up dummies features\n",
    "        # df_features_for_ML = pd.get_dummies(df_features_for_ML, drop_first=False)\n",
    "\n",
    "        #Setting up the train/test split\n",
    "        # y is the target\n",
    "        y = df_features_for_ML1[\"acc_travel_time\"]\n",
    "        # X is everything else\n",
    "        X = df_features_for_ML1.drop([\"acc_travel_time\"],1)\n",
    "        # Split the dataset into two datasets: 70% training and 30% test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "        # Train RF with 10 trees\n",
    "        rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "        #Predict the test dataset\n",
    "        rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "        #Calculate the metrics\n",
    "        MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "        RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "        R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "        headsign1 = headsign1.replace(' ','')\n",
    "        # writing these evaluation scores to a file\n",
    "        f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "        f.write(\"\\nLine: \" + str(data) + '. Head sign stop: ' + headsign1 + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "        f.close()\n",
    "\n",
    "        # save the model to pickle file\n",
    "        filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}_{}.pkl\".format(lineid,headsign1)\n",
    "        pickle.dump(rfc, open(filename, 'wb'))\n",
    "\n",
    "        #Setting up the train/test split\n",
    "        # y is the target\n",
    "        y = df_features_for_ML2[\"acc_travel_time\"]\n",
    "        # X is everything else\n",
    "        X = df_features_for_ML2.drop([\"acc_travel_time\"],1)\n",
    "        # Split the dataset into two datasets: 70% training and 30% test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "        # Train RF with 10 trees\n",
    "        rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "        #Predict the test dataset\n",
    "        rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "        #Calculate the metrics\n",
    "        MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "        RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "        R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "        headsign2 = headsign2.replace(' ','')\n",
    "        # writing these evaluation scores to a file\n",
    "        f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "        f.write(\"\\nLine: \" + str(data) + '. Head sign stop: ' + headsign2 + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "        f.close()\n",
    "\n",
    "        # save the model to pickle file\n",
    "        filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}_{}.pkl\".format(lineid,headsign2)\n",
    "        pickle.dump(rfc, open(filename, 'wb'))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Select the features we need for modeling and Copy the original version of dataset\n",
    "#     df_features_for_ML = df_stoptimes_merged_cleaned[['stop_sequence','stop_headsign','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']]\n",
    "#     df_features_for_ML['stop_headsign'].astype('category')\n",
    "\n",
    "#     # set up dummies features\n",
    "#     df_features_for_ML = pd.get_dummies(df_features_for_ML, drop_first=False)\n",
    "\n",
    "\n",
    "#     #Setting up the train/test split\n",
    "#     # y is the target\n",
    "#     y = df_features_for_ML[\"acc_travel_time\"]\n",
    "#     # X is everything else\n",
    "#     X = df_features_for_ML.drop([\"acc_travel_time\"],1)\n",
    "#     # Split the dataset into two datasets: 70% training and 30% test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "#     # Train aka fit, a model using all continuous and categorical features.\n",
    "#     linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the test dataset\n",
    "#     linreg_predictions_test = linreg.predict(X_test)\n",
    "\n",
    "#     #Calculate the metrics\n",
    "#     MAE_linreg = metrics.mean_absolute_error(y_test, linreg_predictions_test)\n",
    "#     RMSE_linreg = metrics.mean_squared_error(y_test, linreg_predictions_test)**0.5\n",
    "#     R2_linreg = metrics.r2_score(y_test, linreg_predictions_test)\n",
    "\n",
    "#     # writing these evaluation scores to a file\n",
    "#     f = open(\"GTFS_LinearRegres_Evaluation_08_08.txt\", \"a\")\n",
    "#     f.write(\"\\nLine: \" + str(data) + \". MAE: \" + str(MAE_linreg) + \". RMSE: \" + str(RMSE_linreg) + \". R2 Score: \" + str(R2_linreg) + \".\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # save the model to pickle file\n",
    "#     filename = \"ModelResult_LinearRegression_GTFS/PickleFiles_{}.pkl\".format(lineid)\n",
    "#     pickle.dump(linreg, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "#     # Train RF with 10 trees\n",
    "#     rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the test dataset\n",
    "#     rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "#     #Calculate the metrics\n",
    "#     MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "#     RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "#     R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "#     # writing these evaluation scores to a file\n",
    "#     f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "#     f.write(\"\\nLine: \" + str(data) + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # save the model to pickle file\n",
    "#     filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}.pkl\".format(lineid)\n",
    "#     pickle.dump(linreg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab683dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of headsign:  2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ccc1302a5344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_features_for_ML1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"acc_travel_time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Split the dataset into two datasets: 70% training and 30% test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0m\u001b[1;32m   2176\u001b[0m                                               default_test_size=0.25)\n\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1858\u001b[0m             \u001b[0;34m'With n_samples={}, test_size={} and train_size={}, the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#df_stoptimes_merged_cleaned = pd.read_csv('df_stoptimes_route_{}.csv'.format(lineid))\n",
    "\n",
    "# if len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()) == 1:\n",
    "#     print('Only one headsign or Nan value in headsign column.')\n",
    "    \n",
    "#     # #Select the features we need for modeling and Copy the original version of dataset\n",
    "#     df_features_for_ML1 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']]\n",
    "#     # df_features_for_ML['stop_headsign'].fillna('n/a',inplace=True)\n",
    "#     # df_features_for_ML['temp'].fillna(0, inplace=True)\n",
    "#     # df_features_for_ML['rain'].fillna(0, inplace=True)\n",
    "#     # df_features_for_ML['stop_headsign'].astype('category')\n",
    "\n",
    "#     # # set up dummies features\n",
    "#     # df_features_for_ML = pd.get_dummies(df_features_for_ML, drop_first=False)\n",
    "\n",
    "#     #Setting up the train/test split\n",
    "#     # y is the target\n",
    "#     y = df_features_for_ML1[\"acc_travel_time\"]\n",
    "#     # X is everything else\n",
    "#     X = df_features_for_ML1.drop([\"acc_travel_time\"],1)\n",
    "#     # Split the dataset into two datasets: 70% training and 30% test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "#     # Train RF with 10 trees\n",
    "#     rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the test dataset\n",
    "#     rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "#     #Calculate the metrics\n",
    "#     MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "#     RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "#     R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "#     # writing these evaluation scores to a file\n",
    "#     f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "#     f.write(\"\\nLine: \" + str(data)  + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # save the model to pickle file\n",
    "#     filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}.pkl\".format(lineid)\n",
    "#     pickle.dump(rfc, open(filename, 'wb'))\n",
    "    \n",
    "# elif len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()) > 1:\n",
    "#     print('Length of headsign: ', len(df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()))\n",
    "\n",
    "#     headsign1 = df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()[0]\n",
    "#     headsign2 = df_stoptimes_merged_cleaned['stop_headsign'].unique().tolist()[1]\n",
    "\n",
    "#     #Select the features we need for modeling and Copy the original version of dataset\n",
    "#     df_features_for_ML1 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']].loc[df_stoptimes_merged_cleaned['stop_headsign']==headsign1]\n",
    "#     df_features_for_ML2 = df_stoptimes_merged_cleaned[['stop_sequence','acc_travel_time','hour_of_day','day_of_week','month_of_year','temp','rain']].loc[df_stoptimes_merged_cleaned['stop_headsign']==headsign2]\n",
    "#     # df_features_for_ML['stop_headsign'].fillna('n/a',inplace=True)\n",
    "#     # df_features_for_ML['temp'].fillna(0, inplace=True)\n",
    "#     # df_features_for_ML['rain'].fillna(0, inplace=True)\n",
    "#     # df_features_for_ML['stop_headsign'].astype('category')\n",
    "\n",
    "#     # # set up dummies features\n",
    "#     # df_features_for_ML = pd.get_dummies(df_features_for_ML, drop_first=False)\n",
    "\n",
    "#     #Setting up the train/test split\n",
    "#     # y is the target\n",
    "#     y = df_features_for_ML1[\"acc_travel_time\"]\n",
    "#     # X is everything else\n",
    "#     X = df_features_for_ML1.drop([\"acc_travel_time\"],1)\n",
    "#     # Split the dataset into two datasets: 70% training and 30% test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "#     # Train RF with 10 trees\n",
    "#     rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the test dataset\n",
    "#     rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "#     #Calculate the metrics\n",
    "#     MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "#     RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "#     R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "#     headsign1 = headsign1.replace(' ','')\n",
    "#     # writing these evaluation scores to a file\n",
    "#     f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "#     f.write(\"\\nLine: \" + str(data) + '. Head sign stop: ' + headsign1 + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # save the model to pickle file\n",
    "#     filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}_{}.pkl\".format(lineid,headsign1)\n",
    "#     pickle.dump(rfc, open(filename, 'wb'))\n",
    "\n",
    "#     #Setting up the train/test split\n",
    "#     # y is the target\n",
    "#     y = df_features_for_ML2[\"acc_travel_time\"]\n",
    "#     # X is everything else\n",
    "#     X = df_features_for_ML2.drop([\"acc_travel_time\"],1)\n",
    "#     # Split the dataset into two datasets: 70% training and 30% test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,  test_size=0.3)\n",
    "\n",
    "\n",
    "#     # Train RF with 10 trees\n",
    "#     rfc = RandomForestRegressor(n_estimators=10, max_features='auto', oob_score=True, random_state=1).fit(X_train, y_train)\n",
    "\n",
    "#     #Predict the test dataset\n",
    "#     rfc_predictions_test = rfc.predict(X_test)\n",
    "\n",
    "#     #Calculate the metrics\n",
    "#     MAE_rfc = metrics.mean_absolute_error(y_test, rfc_predictions_test)\n",
    "#     RMSE_rfc = metrics.mean_squared_error(y_test, rfc_predictions_test)**0.5\n",
    "#     R2_rfc = metrics.r2_score(y_test, rfc_predictions_test)\n",
    "\n",
    "#     headsign2 = headsign2.replace(' ','')\n",
    "#     # writing these evaluation scores to a file\n",
    "#     f = open(\"GTFS_RandomForest_Evaluation_08_08.txt\", \"a\")\n",
    "#     f.write(\"\\nLine: \" + str(data) + '. Head sign stop: ' + headsign2 + \". MAE: \" + str(MAE_rfc) + \". RMSE: \" + str(RMSE_rfc) + \". R2 Score: \" + str(R2_rfc) + \".\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     # save the model to pickle file\n",
    "#     filename = \"ModelResult_RandomForest_GTFS/PickleFiles_{}_{}.pkl\".format(lineid,headsign2)\n",
    "#     pickle.dump(rfc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e91bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
